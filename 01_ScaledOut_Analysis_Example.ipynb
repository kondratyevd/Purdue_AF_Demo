{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from math import pi\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import get_client\n",
    "from dask.distributed import wait\n",
    "\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import vector\n",
    "vector.register_awkward()\n",
    "\n",
    "from ROOT import TCanvas, TH1F, TLegend, TFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking, if the voms proxy is set\n",
    "\n",
    "Set the valid proxy, as we are going to access the data using xrootd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject   : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=piperov/CN=422973/CN=Stefcho Piperov/CN=3014286933\n",
      "issuer    : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=piperov/CN=422973/CN=Stefcho Piperov\n",
      "identity  : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=piperov/CN=422973/CN=Stefcho Piperov\n",
      "type      : RFC compliant proxy\n",
      "strength  : 1024 bits\n",
      "path      : /home/spiperov/x509up_u638764\n",
      "timeleft  : 167:07:38\n"
     ]
    }
   ],
   "source": [
    "!voms-proxy-info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the dask cluster\n",
    "\n",
    "Please follow the instructions carefully for setting up the dask cluster and pass the ip address and port number in the below cell\n",
    "\n",
    "Please note the ip-address and the port for the dashboard of the scheduler and connect a browser there to monitor your dask cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to cluster!\n",
      "<Client: 'tcp://128.211.149.206:40696' processes=10 threads=10, memory=36.30 GiB>\n"
     ]
    }
   ],
   "source": [
    "node_ip = \"128.211.149.206\"  ## this is the ip-address of the cluster\n",
    "slurm_port = \"40696\"        ## port to communicate to the scheduler\n",
    "client = Client(f\"{node_ip}:{slurm_port}\")\n",
    "print(f\"Connected to cluster!\")\n",
    "print(client)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading input files using xrootd\n",
    "\n",
    "It might be useful to add this line to your bash initialization files\n",
    "\n",
    "`export XRD_REQUESTTIMEOUT=300`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrootdserver = 'root://xrootd.rcac.purdue.edu/'\n",
    "\n",
    "\n",
    "files_dy =    !xrdfs root://xrootd.rcac.purdue.edu/ ls -R /store/mc/RunIISummer20UL18NanoAODv2/DYJetsToLL_M-50_TuneCP5_13TeV-madgraphMLM-pythia8/NANOAODSIM/106X_upgrade2018_realistic_v15_L1v1-v1/|grep '.root$'\n",
    "\n",
    "files_tt =    !xrdfs root://xrootd.rcac.purdue.edu/ ls -R /store/mc/RunIISummer20UL18NanoAODv2/TTTo2L2Nu_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_upgrade2018_realistic_v15_L1v1-v1/|grep '.root$'\n",
    "\n",
    "files_data =  !xrdfs root://xrootd.rcac.purdue.edu/ ls -R /store/data/Run2018D/DoubleMuon/NANOAOD/UL2018_MiniAODv2_NanoAODv9_GT36-v1/|grep '.root$'  \n",
    "\n",
    "all_files = {\n",
    "    \"dy\": [xrootdserver+ str(f) for f in files_dy],\n",
    "    \"tt\": [xrootdserver+ str(f) for f in files_tt],\n",
    "    \"data\": [xrootdserver+ str(f) for f in files_data]\n",
    "}\n",
    "\n",
    "\n",
    "max_events = 5000\n",
    "\n",
    "## Set max_events to -1 to run over entire dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##if the files are present locally, you can read directly from the filesystem\n",
    "\n",
    "# all_files = []\n",
    "# path = \"/mnt/hadoop/store/mc/RunIISummer20UL18NanoAODv2/DYJetsToLL_M-50_TuneCP5_13TeV-madgraphMLM-pythia8/NANOAODSIM/106X_upgrade2018_realistic_v15_L1v1-v1/50000/\"\n",
    "# for f in glob.glob(path+\"*.root\"):\n",
    "#     all_files.append(f)\n",
    "    \n",
    "# all_files1 = []\n",
    "# path = \"/mnt/hadoop//store/mc/RunIISummer20UL18NanoAODv2/TTTo2L2Nu_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_upgrade2018_realistic_v15_L1v1-v1/270000/\"\n",
    "# for f in glob.glob(path+\"*.root\"):\n",
    "#     all_files1.append(f)\n",
    "    \n",
    "# all_files2 = []\n",
    "# path = \"/mnt/hadoop//store/data/Run2018D/DoubleMuon/NANOAOD/UL2018_MiniAODv2_NanoAODv9_GT36-v1/2820000/\"\n",
    "# for f in glob.glob(path+\"*.root\"):\n",
    "#     all_files2.append(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lets apply some selections\n",
    "\n",
    "Here we are selecting two oppositely charged muons and saving the information in dask dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processor(file):\n",
    "    import awkward as ak\n",
    "    import vector\n",
    "    vector.register_awkward()\n",
    "    \n",
    "\n",
    "    tree = uproot.open(file)[\"Events\"].arrays(entry_start=0, entry_stop=max_events)\n",
    "\n",
    "    muons = ak.Array(\n",
    "        ak.zip(\n",
    "            {\n",
    "                \"nmu\": tree[\"nMuon\"],\n",
    "                \"pt\": tree[\"Muon_pt\"],\n",
    "                \"eta\": tree[\"Muon_eta\"],\n",
    "                \"charge\": tree[\"Muon_charge\"],\n",
    "                \"id\": tree[\"Muon_isGlobal\"],\n",
    "                \"phi\": tree[\"Muon_phi\"],\n",
    "                \"mass\": tree[\"Muon_mass\"],\n",
    "                \"met\": tree[\"MET_pt\"]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "    muon_mask = (muons.id==1) & (muons.pt > 20) & (abs(muons.eta)<2.4)\n",
    "    good_muons = muons[muon_mask]\n",
    "    two_muons = good_muons[(ak.sum(muon_mask, axis=-1)==2)]\n",
    "    opp_muons = two_muons.charge[:,0] != two_muons.charge[:,1]\n",
    "\n",
    "    two_opp_good_muons = two_muons[opp_muons]\n",
    "\n",
    "    mu_p4 = ak.Array(\n",
    "        ak.zip(\n",
    "            {\n",
    "                \"pt\":two_opp_good_muons.pt,\n",
    "                \"eta\":two_opp_good_muons.eta,\n",
    "                \"phi\":two_opp_good_muons.phi,\n",
    "                \"mass\":two_opp_good_muons.mass\n",
    "            }\n",
    "        ), with_name = \"Momentum4D\"\n",
    "    )\n",
    "\n",
    "    dimuon_p4 = mu_p4[:,0] + mu_p4[:,1]\n",
    "\n",
    "    mu_cols = [\"pt\", \"eta\", \"phi\"]\n",
    "    \n",
    "    ## Converting awkward arrays to panda dataframes\n",
    "    df_mu1 = ak.to_pandas(two_opp_good_muons[:,0][mu_cols])\n",
    "    df_mu1 = df_mu1.add_prefix('mu1_')\n",
    "    \n",
    "    df_mu2 = ak.to_pandas(two_opp_good_muons[:,1][mu_cols])\n",
    "    df_mu2 = df_mu2.add_prefix('mu2_')\n",
    "    \n",
    "    df_met = pd.DataFrame(two_opp_good_muons.met[:,0], columns=['MET'])\n",
    "\n",
    "    df = pd.concat([df_mu1, df_mu2, df_met], axis=1)\n",
    "    df[\"dimuon_mass\"] = dimuon_p4.M\n",
    "    \n",
    "    df = df.astype(float)\n",
    "    \n",
    "    ## Converting the panda dataframes to dask dataframes to distribute to workers\n",
    "    \n",
    "    return dd.from_pandas(df, npartitions=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending the task to the worker nodes\n",
    "\n",
    "Here we are sending the task using dask distributor to the worker nodes to apply selections on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "futures = {}\n",
    "for dataset, paths in all_files.items():\n",
    "    futures[dataset] = client.map(processor, paths)\n",
    "\n",
    "for dataset, paths in all_files.items():\n",
    "    if(wait(futures[dataset])):\n",
    "        print(f\"{dataset}: DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "We are gathering the output of the previous step from each worker node.\n",
    "Remember: The output is stored as dask dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for key, future in futures.items():\n",
    "    results[key] = dd.concat(client.gather(future))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the invariant mass of the two muons\n",
    "\n",
    "While plotting, the dask dataframes will be computed to get the real values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot dimuon invariant mass'''\n",
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "\n",
    "for key, res in results.items():\n",
    "    mass = res.dimuon_mass\n",
    "    \n",
    "    if(key=='dy'):\n",
    "        plt.hist(mass, bins=150, range=[0, 500], histtype=\"step\",linewidth=2, color='blue', label='DY+Jets')\n",
    "    elif(key == 'tt'):\n",
    "        plt.hist(mass, bins=150, range=[0,500], histtype='step',linewidth=2, color='orange', label='ttbar')\n",
    "    else:\n",
    "        n, bins, patches = plt.hist(mass, bins=150, range=[0,500], histtype='step',linewidth=0)\n",
    "\n",
    "        \n",
    "errory = np.sqrt(n)\n",
    "plt.errorbar(np.linspace(0,500,150), n,yerr= errory, fmt='o', markersize=3, color='k', label='Data')\n",
    "    \n",
    "plt.title('Dimuon invariant mass')\n",
    "plt.xlabel('Dimuon invariant mass [GeV]')\n",
    "plt.ylabel('Events')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig(f\"dimuon_mass_parallel.pdf\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DNN using PyTorch to select the Drell-Yan events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_features = ['mu1_pt', 'mu1_eta', 'mu2_pt', 'mu2_eta', 'dimuon_mass','MET']\n",
    "\n",
    "df_sig = results['dy'][load_features]\n",
    "df_bkg = results['tt'][load_features]\n",
    "\n",
    "df_sig[\"label\"] = 1.0\n",
    "df_bkg[\"label\"] = 0.0\n",
    "\n",
    "dataset = dd.concat([df_sig, df_bkg], ignore_index=True)\n",
    "\n",
    "##Sampling the training and validation data\n",
    "\n",
    "train_data, val_data = dataset.random_split([0.7, 0.3], shuffle=True)\n",
    "\n",
    "\n",
    "train_labels = train_data[\"label\"].compute().values\n",
    "val_labels = val_data[\"label\"].compute().values\n",
    "\n",
    "train = train_data.drop(columns=[\"label\"]).compute()\n",
    "train = train.dropna().values\n",
    "\n",
    "val = val_data.drop(columns=[\"label\"]).compute()\n",
    "val = val.dropna().values\n",
    "\n",
    "train_size = len(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Lets train a network and apply some selections'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"****Model will run on\", device)\n",
    "\n",
    "# Seed\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ELU())\n",
    "            #layers.append(nn.Dropout(p=0.1))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weight    \n",
    "    def forward(self, x):\n",
    "        out = self.layers[0](x)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            out = self.layers[i](out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(load_features)\n",
    "hidden_sizes = [16, 8]\n",
    "learning_rate = 0.001\n",
    "num_classes = 1\n",
    "num_epochs =  10\n",
    "batch_size =  512\n",
    "\n",
    "\n",
    "model = NeuralNet(input_size, hidden_sizes, num_classes).to(device)\n",
    "model = model.float()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "decayRate = 0.6\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "total_step = train_size\n",
    "\n",
    "print(\"RUNNING... \")\n",
    "pbar = None\n",
    "pbar = tqdm.tqdm(range(num_epochs), position=0, leave=True, colour=\"green\")\n",
    "for epoch in pbar:\n",
    "    mean_loss = 0\n",
    "    tot_wgt = 0\n",
    "    val_mean_loss = 0\n",
    "    val_tot_wgt = 0\n",
    "    for i in range(int(train_size/batch_size)):\n",
    "        # Move tensors to the configured device\n",
    "        data = torch.from_numpy(train[i*batch_size: (i+1)*batch_size]).to(device)\n",
    "        label = torch.from_numpy(train_labels[i*batch_size: (i+1)*batch_size].reshape((batch_size,1))).to(device)\n",
    "\n",
    "        outputs = model(data.float())\n",
    "        loss = criterion(outputs, label.float())\n",
    "        weight_loss = loss\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        weight_loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        mean_loss += weight_loss.mean().item()*batch_size\n",
    "        if i%4 == 0:\n",
    "            j = int(i/4)\n",
    "            val_data = torch.from_numpy(val[j*batch_size: (j+1)*batch_size]).to(device)\n",
    "            val_label = torch.from_numpy(val_labels[j*batch_size: (j+1)*batch_size].reshape(val_labels[j*batch_size: (j+1)*batch_size].shape[0],1)).to(device)\n",
    "            val_outputs = model(val_data.float())\n",
    "\n",
    "\n",
    "            val_loss = criterion(val_outputs, val_label.float())\n",
    "\n",
    "        if (i+1) % 150 == 0:\n",
    "            #print ('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}'\n",
    "            #       .format(epoch+1, num_epochs, i+1, int(total_step/batch_size), mean_loss, val_loss))\n",
    "            mean_loss=0\n",
    "            tot_wgt=0\n",
    "            val_mean_loss=0\n",
    "            val_tot_wgt=0\n",
    "    my_lr_scheduler.step()\n",
    "    pbar.set_postfix({\"val_loss\": val_loss.item()})\n",
    "    #pbar.update()\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n",
    "\n",
    "'''The model has been saved. Lets now evaluate the results'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model output\n",
    "\n",
    "Training of the model is done locally as it needs the full chunk of training data, but the evaluation can be done on the worker nodes by sending model on each worker node and then getting the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading the model'''\n",
    "\n",
    "df_sig = results['dy'][load_features]\n",
    "df_bkg = results['tt'][load_features]\n",
    "df_data = results['data'][load_features]\n",
    "\n",
    "\n",
    "def inference(df):\n",
    "    device = torch.device('cpu')\n",
    "    model = NeuralNet(6, [16, 8], 1).to(device)\n",
    "    model.load_state_dict(torch.load(\"model.ckpt\", map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    df = torch.from_numpy(df.compute().values).to(device).float()\n",
    "    scores = model(df) \n",
    "    scores = scores.cpu().detach().numpy()\n",
    "    return scores.ravel()  \n",
    "\n",
    "##As the evaluation data is big, first the data is scattered on the nodes and then evaluation\n",
    "## is performed using dask\n",
    "\n",
    "scattered_data = client.scatter([df_sig, df_bkg, df_data])\n",
    "futures = client.map(inference, scattered_data)\n",
    "dnn_sig, dnn_bkg, dnn_data = client.gather(futures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Checking the DNN Score'''\n",
    "bins = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "\n",
    "plt.hist(dnn_sig, bins, alpha=0.3, label='sig')\n",
    "plt.hist(dnn_bkg, bins, alpha=0.3, label='bkg')\n",
    "plt.xlabel('DNN Score')\n",
    "plt.ylabel('Events')\n",
    "plt.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets check the dimuon mass after selection with DNN based discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DNN_CUT = 0.5\n",
    "\n",
    "dy_mass_dnn = df_sig[\"dimuon_mass\"].compute()[dnn_sig>DNN_CUT]\n",
    "tt_mass_dnn = df_bkg[\"dimuon_mass\"].compute()[dnn_bkg>DNN_CUT]\n",
    "data_mass_dnn = df_data[\"dimuon_mass\"].compute()[dnn_data>DNN_CUT]\n",
    "\n",
    "\n",
    "'''Plot dimuon invariant mass'''\n",
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "plt.hist(dy_mass_dnn, bins=150, range=[0,500], histtype='step',linewidth=2, color='blue', label='DY+Jets')\n",
    "plt.hist(tt_mass_dnn, bins=150, range=[0,500], histtype='step',linewidth=2, color='orange', label='ttbar')\n",
    "n, bins, patches = plt.hist(data_mass_dnn, bins=150, range=[0,500], histtype='step',linewidth=0)\n",
    "\n",
    "errory = np.sqrt(n)\n",
    "plt.errorbar(np.linspace(0,500,150), n,yerr= errory, fmt='o', markersize=3, color='k', label='Data')\n",
    "\n",
    "plt.title('Dimuon invariant mass')\n",
    "plt.xlabel('Dimuon invariant mass [GeV]')\n",
    "plt.ylabel('Events')\n",
    "#plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig(f\"dimuon_mass_dnncut_parallel.pdf\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We see that the selected data events are populated with Drell-Yan events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_ML_DASK_GPU [AF]",
   "language": "python",
   "name": "python_ml_dask_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
